---
jupyter:
  jupytext:
    text_representation:
      extension: .Rmd
      format_name: rmarkdown
      format_version: '1.2'
      jupytext_version: 1.13.0
  kernelspec:
    display_name: Python 3
    language: python
    name: python3
---

```{python}
import numpy as np
import matplotlib.pyplot as plt

import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
```

```{python}
# !pip install tensorflow
```

```{python}

import math
import numpy as np
import tensorflow as tf
import pandas as pd
import os
import matplotlib.pyplot as plt
import pickle
import datetime
from tensorflow.python.keras import backend as K
from tensorflow.keras.layers import TimeDistributed

tf.compat.v1.enable_eager_execution()


class Model(tf.keras.Model):



    def __init__(self,name):
        super(Model, self).__init__()

        self.test_preds = None
        self.train_preds = None
        self.test_name = name
        

    def model(self,**kwargs):
        alpha = 0.05

        self.conv1 = TimeDistributed(tf.keras.layers.DepthwiseConv2D((1,9), 1, input_shape=(None,1,10,300,1),use_bias=False, depth_multiplier=12,dtype=tf.float32,data_format='channels_last'))
        self.batch1 = TimeDistributed(tf.keras.layers.BatchNormalization())
        self.relu1 = TimeDistributed(tf.keras.layers.LeakyReLU(alpha=alpha, dtype=tf.float32))
        self.max1 = TimeDistributed(tf.keras.layers.MaxPool2D((1,4), dtype=tf.float32,data_format='channels_last'))
        self.lstm = tf.keras.layers.LSTM(10)

        self.flatten1 = TimeDistributed(tf.keras.layers.Flatten(dtype=tf.float32))
        self.flatten2 = TimeDistributed(tf.keras.layers.Flatten(dtype=tf.float32))
        
        
        self.flatten3 = tf.keras.layers.Flatten(dtype=tf.float32)


        self.drop1 = tf.keras.layers.Dropout(0.2, dtype=tf.float64)
        self.d1= tf.keras.layers.Dense(364, dtype=tf.float64)
        self.relu1 = tf.keras.layers.LeakyReLU(alpha=alpha, dtype=tf.float64)

        self.drop2 = tf.keras.layers.Dropout(0.2, dtype=tf.float64)
        self.d2= tf.keras.layers.Dense(128, dtype=tf.float64)
        self.relu2 = tf.keras.layers.LeakyReLU(alpha=alpha, dtype=tf.float64)
        
        self.final = tf.keras.layers.Dense(7,activation='sigmoid', dtype=tf.float64)

        self.optimizer = tf.keras.optimizers.Adam()
        self.train_loss = tf.metrics.Mean(name='train_loss')
        self.train_accuracy = tf.keras.metrics.CategoricalAccuracy(name='train_accuracy')
        self.test_loss = tf.keras.metrics.Mean(name='test_loss')
        self.test_accuracy = tf.keras.metrics.CategoricalAccuracy(name='test_accuracy')
        self.loss_object = tf.keras.losses.CategoricalCrossentropy(from_logits=False)

        #Compile function required for exporting
        self.compile(optimizer=self.optimizer,loss= self.loss_object,metrics=self.test_accuracy)
        self.graph_has_been_written = False
        self.ckpt = tf.train.Checkpoint(step=tf.Variable(1), optimizer=self.optimizer, net=self)
        self.manager = tf.train.CheckpointManager(self.ckpt, '/', max_to_keep=3)
        self.graph_log_dir = 'logs\\gradient_tape\\' + self.test_name + '\\graph'


    def call(self, x):
        #x = tf.pad(x, paddings, "SYMMETRIC")#,constant_values=mean)
        #Convolution Tower 1 (Time domain)
        x1 = self.conv1(x)
        x1 = self.batch1(x1)
        x1 = self.relu1(x1)
        x1 = self.max1(x1)
        x1 = self.flatten1(x1)
        x1 = self.lstm(x1)

        x = self.drop1(x1)
        x = self.d1(x)
        x = self.relu1(x)
        x = self.drop2(x)
        x = self.d2(x)
        x = self.relu2(x)

        x = self.final(x)


        #x = tf.nn.softmax(x)

        return x

    @tf.function
    def train_step(self,inputs, labels):
        with tf.GradientTape() as tape:
            predictions = self(inputs, training=True)
            loss= self.loss_object(labels, predictions)

        gradients = tape.gradient(loss, self.trainable_variables)

        self.optimizer.apply_gradients(zip(gradients, self.trainable_variables))

        self.train_loss(loss)
        self.train_accuracy(labels, predictions)
        return gradients


    @tf.function
    def test_step(self,inputs, labels):
        batch_size = 100
        predictions = predictions = self(inputs[:batch_size], training=False)

        for batch in range(1,int(np.ceil(inputs.shape[0]/batch_size))):
            prediction = self(inputs[batch*batch_size:(batch+1)*batch_size], training=False)
            predictions = tf.concat([predictions,prediction],0)

        t_loss = self.loss_object(labels, predictions)
        #t_loss= tf.keras.losses.KLD(labels, predictions)

        self.test_loss(t_loss)
        self.test_accuracy(labels, predictions)
        return predictions

    def train(self,**kwargs):
        #Check if PC has GPU and run on it if so
        print("GPU: ",tf.test.is_gpu_available())
        if tf.test.is_gpu_available():
            gpus = tf.config.experimental.list_physical_devices('GPU')
            try:
                # Currently, memory growth needs to be the same across GPUs
                for gpu in gpus:
                    tf.config.experimental.set_memory_growth(gpu, True)
                    logical_gpus = tf.config.experimental.list_logical_devices('GPU')
                    print(len(gpus), "Physical GPUs,", len(logical_gpus), "Logical GPUs")
            except RuntimeError as e:
                # Memory growth must be set before GPUs have been initialized
                print(e)
            with tf.device('/GPU:' + str(0)):
                self.train_loop(**kwargs)


        else:
            self.train_loop(**kwargs)


    def train_loop(self,dataTrain,**kwargs):

        #Get Training Settings from kwargs
        EPOCHS = kwargs.pop('steps')
        batch_size = kwargs.pop('batch_size')
        logging_frequency = kwargs.pop('logging_frequency')
        #self.ckpt.restore(self.manager.latest_checkpoint)

        #Define tensorbaord log directories
        current_time = datetime.datetime.now().strftime("%Y%m%d-%H%M%S")
        train_log_dir = 'logs/gradient_tape/' + self.test_name  + '/train'
        test_log_dir = 'logs/gradient_tape/' + self.test_name  + '/test'
        cm_log_dir = 'logs/gradient_tape/' + self.test_name  + '/confusion_matrix'
        gr_log_dir = 'logs/gradient_tape/' + self.test_name  + '/gradients'
        dat_log_dir = 'logs/gradient_tape/' + self.test_name  + '/details'
        model_log_dir = 'logs/gradient_tape/' + self.test_name + '/model'

        #Initialise tensrobaord writers
        train_summary_writer = tf.summary.create_file_writer(train_log_dir)
        test_summary_writer = tf.summary.create_file_writer(test_log_dir)
        cm_summary_writer = tf.summary.create_file_writer(cm_log_dir)
        gr_summary_writer = tf.summary.create_file_writer(gr_log_dir)
        dat_summary_writer = tf.summary.create_file_writer(dat_log_dir)
        model_summary_writer = tf.summary.create_file_writer(model_log_dir)

        for epoch in range(EPOCHS):

            # Reset the metrics at the start of the next epoch
            self.train_loss.reset_states()
            self.train_accuracy.reset_states()
            self.test_loss.reset_states()
            self.test_accuracy.reset_states()


            for step in range(0,math.ceil(dataTrain.train_samples.shape[0]/batch_size)):
                batch_x , batch_y = dataTrain.next_batch(batch_size)

                if self.graph_has_been_written == False:
                    tf.summary.trace_on(graph=True,profiler=True)

                gradients = self.train_step(batch_x, batch_y.values)

                if self.graph_has_been_written == False:
                    graph_writer = tf.summary.create_file_writer(self.graph_log_dir)

                    with graph_writer.as_default():
                        tf.summary.trace_export(name=self.graph_log_dir,step=0,profiler_outdir=self.graph_log_dir)
                    graph_writer.flush()
                    self.graph_has_been_written = True
                    print("Wrote eager graph to", self.graph_log_dir)


                #tf.summary.trace_off()
            with tf.device('/CPU:0'):
                with train_summary_writer.as_default():
                    tf.summary.scalar('loss', self.train_loss.result(), step=int(self.ckpt.step))
                    tf.summary.scalar('accuracy', self.train_accuracy.result(), step=int(self.ckpt.step))

            self.test_preds = self.test_step(dataTrain.test_samples, dataTrain.test_labels.values)

            with tf.device('/CPU:0'):
                with test_summary_writer.as_default():
                    tf.summary.scalar('loss', self.test_loss.result(), step=int(self.ckpt.step))
                    tf.summary.scalar('accuracy', self.test_accuracy.result(), step=int(self.ckpt.step))


            template = 'Epoch {}, Loss: {}, Accuracy: {}, Test Loss: {}, Test Accuracy: {}'
            print(template.format(epoch+1,
                                    self.train_loss.result(),
                                    self.train_accuracy.result()*100,
                                    self.test_loss.result(),
                                    self.test_accuracy.result()*100))

            #network.update_history(steps = epoch,metrics = [self.test_loss.result(),self.train_loss.result(),self.train_accuracy.result(),self.test_accuracy.result()],headers = ['step','test_loss','train_loss','training_accuracy','test_accuracy'])
            self.ckpt.step.assign_add(1)

            #if int(self.ckpt.step) % 5 == 0:
            #    #save_path = self.manager.save()
            #    print("Saved checkpoint for step {}: {}".format(int(self.ckpt.step), save_path))


            if int(self.ckpt.step) % 10 == 0:
                #weights = [w for w in self.trainable_weights if 'dense' in w.name and 'bias' in w.name]
                #loss = t_loss
                #optimizer = self.optimizer
                #gradients = optimizer.get_gradients(loss, weights)
                co = 0
                for t in gradients:
                    with gr_summary_writer.as_default():
                        #tf.summary.histogram(str(co), data=t,step=int(self.ckpt.step) )
                        co = co+1

    def test_model(self,inputs,x2,y_true):

        self.train_loss.reset_states()
        self.train_accuracy.reset_states()
        self.test_loss.reset_states()
        self.test_accuracy.reset_states()
        self.test_preds = self.test_step(inputs,x2, y_true)
        template = 'Test Loss: {}, Test Accuracy: {}'
        print(template.format(self.test_loss.result(),
                            self.test_accuracy.result()*100))

    def exportModel(self,exportPath):

        return 0
```

```{python}
from sklearn.model_selection import train_test_split

class train:
    def __init__(self,feats,labels,split=0.3):
    
        train, test = train_test_split(labels.index, test_size=split)
        print(train.shape)
        print(test.shape)
        self.train = train
        self.test = test
        self.train_samples = feats.loc[train].values
        self.test_samples = feats.loc[test].values.reshape(test.shape[0],10,1,300,1) 
        self.train_labels = pd.get_dummies(labels.loc[train])
        self.test_labels = pd.get_dummies(labels.loc[test])
        self.i = 0
    
    def next_batch(self, batch_size=100):
        """returns a dictionary batch of samples(batch_size,250,3) with window size as the key, labels(batchsize,22)"""# Note that the 100 dimension in the reshape call is set by an assumed batch size of 100
        x = {}
        x = self.train_samples[self.i:self.i+batch_size].reshape(batch_size,10,1,300,-1) 
        y = self.train_labels[self.i:self.i+batch_size]#.astype(np.float32)# y_true labels
        self.i = (self.i +  x.shape[0]) % self.train_samples.shape[0]
        return x, y
```

```{python}
ml = Model('test')
```

```{python}
import zlib 
import pickle
def decompress(obj):
    return pickle.loads(zlib.decompress(obj))

l = []
f = []
for i in range(1,6):

    with open('feat_{}.pklz'.format(str(i)),'rb') as fi:
        f.append(decompress(fi.read()))

    with open('labels_{}.pklz'.format(str(i)),'rb') as fi:
        l.append(decompress(fi.read()))

```

```{python}
feats = pd.concat(f).reset_index(drop=True)
labels = pd.concat(l).reset_index(drop=True)
```

```{python}
labels
```

```{python}
feats
```

```{python}
feats = feats.div(feats.max(axis=1), axis=0)
feats = feats.fillna(0)
```

```{python}
feats
```

```{python}
labels
```

```{python}
dataTrain = train(feats,labels['cT'])
```

```{python}
pd.get_dummies(dataTrain.test_labels)
```

```{python}

```

```{python}
mm = ml.model()
```

```{python}
ml.train_loop(dataTrain,steps=300,batch_size=10,logging_frequency=2)
```

```{python}
ml.train_loop(dataTrain,steps=100,batch_size=10,logging_frequency=2)
```

```{python}
ml.train_loop(dataTrain,steps=100,batch_size=10,logging_frequency=2)
```

```{python}

```
